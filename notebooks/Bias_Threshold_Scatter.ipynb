{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code - Section \"Who\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "from descartes import PolygonPatch\n",
    "import mapclassify as mc\n",
    "import fiona\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the traffic data\n",
    "df_police = pd.read_csv(\"ca_san_francisco_2020_04_01.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subset df describing the location of each traffic stop\n",
    "locations = pd.DataFrame({'lat': df_police['lat'], 'lng': df_police['lng']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of those pesky NaN's\n",
    "locations.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the shapefile taken from San Francisco's city website:\n",
    "    #https://data.sfgov.org/Geographic-Locations-and-Boundaries/SF-Find-Neighborhoods/pty2-tcw4\n",
    "#Find the boundaries of the map for uploading the shapefile\n",
    "shp = fiona.open(\"shapefile/SFFind_Neighborhoods.shp\")\n",
    "bds = shp.bounds\n",
    "shp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Basemap(\n",
    "    projection='tmerc',\n",
    "    lon_0=-122.44229,\n",
    "    lat_0=37.7756435,\n",
    "    ellps = 'WGS84',\n",
    "    llcrnrlon=bds[0],\n",
    "    llcrnrlat=bds[1],\n",
    "    urcrnrlon=bds[2],\n",
    "    urcrnrlat=bds[3],\n",
    "    lat_ts=0,\n",
    "    resolution='i',\n",
    "    suppress_ticks=True\n",
    ")\n",
    "\n",
    "m.readshapefile('shapefile/SFFind_Neighborhoods', 'SF',\n",
    "               color = 'none',\n",
    "               zorder = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create d_map dataframe\n",
    "df_map = pd.DataFrame({\n",
    "    #creates the polygon representing each neighborhood\n",
    "    'poly': [Polygon(xy) for xy in m.SF],\n",
    "    #corresponding neighborhood\n",
    "    'ward_name': [ward['name'] for ward in m.SF_info]})\n",
    "    #\n",
    "df_map['area_m'] = df_map['poly'].map(lambda x: x.area)\n",
    "df_map['area_km'] = df_map['area_m'] / 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did we make it here?\n"
     ]
    }
   ],
   "source": [
    "# Create Point objects in map coordinates from dataframe lon and lat values\n",
    "#takes latitude/longitude data and transforms them into map coordinates\n",
    "map_points = pd.Series(\n",
    "    [Point(m(mapped_x, mapped_y)) for mapped_x, mapped_y in zip(locations['lng'], locations['lat'])])\n",
    "#create a single object that represents all of the points\n",
    "stop_points = MultiPoint(list(map_points.values))\n",
    "\n",
    "#takes all the pre-created territory polygons\n",
    "    #Places them in a \"MultiPolygon\" object\n",
    "        #prep creates a prepared geometric object that has all neighborhoods\n",
    "neighborhoods = [prep(Polygon(i)) for i in list(df_map['poly'].values)]\n",
    "\n",
    "#Turn the above information into a dictionary \n",
    "neighbdict = dict(zip(df_map['ward_name'], neighborhoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-610e1a1533fd>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cats['neighborhood'][j] = neighb\n"
     ]
    }
   ],
   "source": [
    "#Map all of the data to a neighborhood using\n",
    "#WARNING DO NOT RUN TAKES FOREVER!! Updated csv was created to only have to run this once\n",
    "df_police['neighborhood'] = None\n",
    "\n",
    "cats = df_police.copy()\n",
    "for i in range(len(df_map['ward_name'])):\n",
    "    neighb = df_map['ward_name'][i]\n",
    "    for j in range(len(stop_points)):\n",
    "        if neighborhoods[i].contains(stop_points[j]):\n",
    "            cats['neighborhood'][j] = neighb\n",
    "\n",
    "df_props = cats.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't fit on deepnote, but is available on request\n",
    "df_props = df_props.groupby(['neighborhood', 'subject_race']).count()\n",
    "cats.to_csv('Neighborhoods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregrate and organize data into proportion of searches of each neighborhood based on demographics\n",
    "    #Returns dictionary of tuples:  (x, y, z) = (prop white, prop minority demographic, neighborhood)\n",
    "rtpoints = {'black': [], 'asian/pacific islander':[], 'hispanic':[]}\n",
    "\n",
    "for hood in df_map['ward_name']:\n",
    "    for peeps in rtpoints.keys():\n",
    "        try:\n",
    "            minority = df_props.loc[peeps, hood]['district']\n",
    "            white = df_props.loc['white', hood]['district']\n",
    "            whiteprop = white[1]/sum(white)\n",
    "            minprop = minority[1]/sum(minority)\n",
    "            rtpoints[peeps].append((whiteprop, minprop, hood))\n",
    "        except:\n",
    "            print(\"Oops!\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the pretty subplots!\n",
    "fig, ax = plt.subplots(ncols = 3, figsize = (9, 3))\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.suptitle('Bias Threshold of Proportion of Non-White Stops vs. White Stops', ha = 'center', fontsize = 14, y=1.2)\n",
    "\n",
    "colours = {'asian/pacific islander':'#bae4bc', 'black': '#7bccc4', 'hispanic':'#2b8cbe'}\n",
    "colors = ['#7bccc4', '#bae4bc', '#2b8cbe']\n",
    "peoples = ['black', 'asian/pacific islander', 'hispanic']\n",
    "\n",
    "#Generate the three plot skeletons\n",
    "for i in range(0, 3):\n",
    "    ax[i].spines['top'].set_visible(False) \n",
    "    ax[i].spines['right'].set_visible(False) \n",
    "    ax[i].spines['left'].set_linewidth(0.4) \n",
    "    ax[i].spines['bottom'].set_linewidth(.4)\n",
    "    ax[i].set_yticks([0, 0.1, 0.2])\n",
    "    ax[i].set_yticklabels(['0', '0.1', '0.2'], fontsize = 10)\n",
    "    ax[i].set_xticks([0, 0.1, 0.2])\n",
    "    ax[i].set_xticklabels(['0', '0.1', '0.2'], fontsize = 10)\n",
    "    ax[i].set_xlim([0,0.23])\n",
    "    ax[i].set_ylim([0,0.23])\n",
    "\n",
    "#Give each a title\n",
    "for i in range(len(colors)):\n",
    "    ax[i].set_title(f'%s' % peoples[i].title(), loc = 'left', color = colors[i], weight='bold')\n",
    "\n",
    "\n",
    "#print('data for black threshold')\n",
    "#plot the different thresholds\n",
    "for (i, j, k) in rtpoints['black']:\n",
    "    ax[0].scatter(i, j, color = colours['black'], alpha = 0.7, edgecolors= \"gray\")\n",
    "\n",
    "\n",
    "print('data for asian/pacific islander threshold')\n",
    "for (i, j, k) in rtpoints['asian/pacific islander']:\n",
    "    ax[1].scatter(i, j, color = colours['asian/pacific islander'], alpha = 0.7, edgecolors= \"gray\")\n",
    "ax[1].set_xlabel('White Search Rate')\n",
    "xval = .13\n",
    "yval = .15\n",
    "threshold = ax[1].annotate(\"Bias threshold\", xy=[xval,yval], xytext=[xval,yval], color = 'gray', fontsize=8)\n",
    "threshold.set_rotation(47.4)\n",
    "\n",
    "print('data for hispanic threshold')\n",
    "for (i, j, k) in rtpoints['hispanic']:\n",
    "    ax[2].scatter(i, j, color = colours['hispanic'], alpha = 0.3, edgecolors= \"gray\")\n",
    "\n",
    "\n",
    "plt.suptitle('Proportion of Non-White searches vs. White searches', ha = 'center', fontsize = 14, y=1.15, x=.448, weight = 'bold')\n",
    "plt.title('Zoned by Neighborhood', x=-1.92, y=1.13)\n",
    "    \n",
    "[line.plot([0, 0.23], [0, 0.23], '-.', linewidth=0.5, color='gray') for line in ax]\n",
    "plt.savefig('Threshold.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# end of the code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
